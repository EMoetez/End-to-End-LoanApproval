{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CreditCardApprovalPrediction\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+--------------+------+-----------+\n",
      "|     ID|CODE_GENDER|FLAG_OWN_CAR|FLAG_OWN_REALTY|CNT_CHILDREN|AMT_INCOME_TOTAL|NAME_INCOME_TYPE| NAME_EDUCATION_TYPE|NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|DAYS_BIRTH|DAYS_EMPLOYED|FLAG_MOBIL|FLAG_WORK_PHONE|FLAG_PHONE|FLAG_EMAIL|OCCUPATION_TYPE|CNT_FAM_MEMBERS|MONTHS_BALANCE|STATUS|mode_status|\n",
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+--------------+------+-----------+\n",
      "|5111095|          F|           N|              N|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|    -10653|        -3007|         1|              0|         1|         0|     Core staff|            2.0|            -6|     C|          C|\n",
      "|5061564|          F|           N|              Y|           0|        112500.0|       Pensioner|Secondary / secon...|           Married|House / apartment|    -18759|       365243|         1|              0|         0|         0|           NULL|            2.0|           -28|     C|          C|\n",
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+--------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_raw_directory = \"sample/raw\"\n",
    "sample_data = spark.read.parquet(sample_raw_directory)\n",
    "sample_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"ID\":5111095,\"CODE_GENDER\":\"F\",\"FLAG_OWN_CAR\":\"N\",\"FLAG_OWN_REALTY\":\"N\",\"CNT_CHILDREN\":0,\"AMT_INCOME_TOTAL\":112500.0,\"NAME_INCOME_TYPE\":\"Working\",\"NAME_EDUCATION_TYPE\":\"Secondary \\/ secondary special\",\"NAME_FAMILY_STATUS\":\"Married\",\"NAME_HOUSING_TYPE\":\"House \\/ apartment\",\"DAYS_BIRTH\":-10653,\"DAYS_EMPLOYED\":-3007,\"FLAG_MOBIL\":1,\"FLAG_WORK_PHONE\":0,\"FLAG_PHONE\":1,\"FLAG_EMAIL\":0,\"OCCUPATION_TYPE\":\"Core staff\",\"CNT_FAM_MEMBERS\":2.0,\"MONTHS_BALANCE\":-6,\"STATUS\":\"C\",\"mode_status\":\"C\"},{\"ID\":5061564,\"CODE_GENDER\":\"F\",\"FLAG_OWN_CAR\":\"N\",\"FLAG_OWN_REALTY\":\"Y\",\"CNT_CHILDREN\":0,\"AMT_INCOME_TOTAL\":112500.0,\"NAME_INCOME_TYPE\":\"Pensioner\",\"NAME_EDUCATION_TYPE\":\"Secondary \\/ secondary special\",\"NAME_FAMILY_STATUS\":\"Married\",\"NAME_HOUSING_TYPE\":\"House \\/ apartment\",\"DAYS_BIRTH\":-18759,\"DAYS_EMPLOYED\":365243,\"FLAG_MOBIL\":1,\"FLAG_WORK_PHONE\":0,\"FLAG_PHONE\":0,\"FLAG_EMAIL\":0,\"OCCUPATION_TYPE\":null,\"CNT_FAM_MEMBERS\":2.0,\"MONTHS_BALANCE\":-28,\"STATUS\":\"C\",\"mode_status\":\"C\"}]\n",
      "[{\"ID\":5111095,\"CODE_GENDER\":\"F\",\"FLAG_OWN_CAR\":\"N\",\"FLAG_OWN_REALTY\":\"N\",\"CNT_CHILDREN\":0,\"AMT_INCOME_TOTAL\":112500.0,\"NAME_INCOME_TYPE\":\"Working\",\"NAME_EDUCATION_TYPE\":\"Secondary \\/ secondary special\",\"NAME_FAMILY_STATUS\":\"Married\",\"NAME_HOUSING_TYPE\":\"House \\/ apartment\",\"DAYS_BIRTH\":-10653,\"DAYS_EMPLOYED\":-3007,\"FLAG_MOBIL\":1,\"FLAG_WORK_PHONE\":0,\"FLAG_PHONE\":1,\"FLAG_EMAIL\":0,\"OCCUPATION_TYPE\":\"Core staff\",\"CNT_FAM_MEMBERS\":2.0,\"MONTHS_BALANCE\":-6,\"STATUS\":\"C\",\"mode_status\":\"C\"},{\"ID\":5061564,\"CODE_GENDER\":\"F\",\"FLAG_OWN_CAR\":\"N\",\"FLAG_OWN_REALTY\":\"Y\",\"CNT_CHILDREN\":0,\"AMT_INCOME_TOTAL\":112500.0,\"NAME_INCOME_TYPE\":\"Pensioner\",\"NAME_EDUCATION_TYPE\":\"Secondary \\/ secondary special\",\"NAME_FAMILY_STATUS\":\"Married\",\"NAME_HOUSING_TYPE\":\"House \\/ apartment\",\"DAYS_BIRTH\":-18759,\"DAYS_EMPLOYED\":365243,\"FLAG_MOBIL\":1,\"FLAG_WORK_PHONE\":0,\"FLAG_PHONE\":0,\"FLAG_EMAIL\":0,\"OCCUPATION_TYPE\":null,\"CNT_FAM_MEMBERS\":2.0,\"MONTHS_BALANCE\":-28,\"STATUS\":\"C\",\"mode_status\":\"C\"}]\n"
     ]
    }
   ],
   "source": [
    "def convert_parquet_to_json(spark_df):\n",
    "    \"\"\"\n",
    "    Convert a Spark DataFrame to JSON format and print the result.\n",
    "\n",
    "    Parameters:\n",
    "    - spark_df: Spark DataFrame loaded from Parquet.\n",
    "\n",
    "    Returns:\n",
    "    - A list of JSON objects (each row as a dictionary).\n",
    "    \"\"\"\n",
    "    # Convert Spark DataFrame to Pandas DataFrame\n",
    "    pandas_df = spark_df.toPandas()\n",
    "    \n",
    "    # Convert Pandas DataFrame to JSON\n",
    "    json_data = pandas_df.to_json(orient='records')\n",
    "    \n",
    "    # Print the JSON data\n",
    "    print(json_data)\n",
    "    \n",
    "    # Return the JSON data\n",
    "    return json_data\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `sample_raw` is your loaded Spark DataFrame\n",
    "json_output = convert_parquet_to_json(sample_data)\n",
    "print(json_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+---------------+----------+----------+---------------+---------------+--------------+------+-----------+------------------+------------------+---------------------+---------------------+---------------+--------------------+--------------------+\n",
      "|     ID|CODE_GENDER|FLAG_OWN_CAR|FLAG_OWN_REALTY|CNT_CHILDREN|AMT_INCOME_TOTAL|NAME_INCOME_TYPE| NAME_EDUCATION_TYPE|NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|FLAG_MOBIL|FLAG_WORK_PHONE|FLAG_PHONE|FLAG_EMAIL|OCCUPATION_TYPE|CNT_FAM_MEMBERS|MONTHS_BALANCE|STATUS|mode_status|               AGE|    YEARS_EMPLOYED|INCOME_PER_FAM_MEMBER|CREDIT_HISTORY_LENGTH|RECENT_ACTIVITY|            features|     scaled_features|\n",
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+---------------+----------+----------+---------------+---------------+--------------+------+-----------+------------------+------------------+---------------------+---------------------+---------------+--------------------+--------------------+\n",
      "|5111095|          F|           N|              N|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|         1|              0|         1|         0|     Core staff|            2.0|            -6|     C|          C|29.186301369863013| 8.238356164383562|              56250.0|                    0|              0|[56250.0,112500.0...|[0.78590430402864...|\n",
      "|5061564|          F|           N|              Y|           0|        112500.0|       Pensioner|Secondary / secon...|           Married|House / apartment|         1|              0|         0|         0|           NULL|            2.0|           -28|     C|          C|51.394520547945206|1000.6657534246575|              56250.0|                    0|              0|[56250.0,112500.0...|[0.78590430402864...|\n",
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+---------------+----------+----------+---------------+---------------+--------------+------+-----------+------------------+------------------+---------------------+---------------------+---------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from process import DataProcessor  \n",
    "sample_processed_directory = \"sample/processed\"\n",
    "data_processor = DataProcessor()\n",
    "sample_processed = data_processor.process(input_df=sample_data,output_dir=sample_processed_directory)\n",
    "sample_processed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists(\"rf_model\"))  # Should print True if the path exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MSI\\Desktop\\bigdata-20241208T104837Z-001\\bigdata\\Model Development\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "model_path = \"C:/Users/MSI/Desktop/bigdata-20241208T104837Z-001/bigdata/Model_Deployment/rf_model\" \n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "rf_model = RandomForestClassificationModel.load(model_path)\n",
    "print(\"RandomForest model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|       0.0|\n",
      "|       0.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Load the test data\n",
    "test_data_loaded = spark.read.parquet(sample_processed_directory)\n",
    "\n",
    "# Make predictions using the random forest model\n",
    "rf_predictions = rf_model.transform(test_data_loaded)\n",
    "\n",
    "# Show only the predictions (exclude 'label')\n",
    "rf_predictions.select(\"prediction\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-18.1.0-cp310-cp310-win_amd64.whl.metadata (3.4 kB)\n",
      "Downloading pyarrow-18.1.0-cp310-cp310-win_amd64.whl (25.1 MB)\n",
      "   ---------------------------------------- 25.1/25.1 MB 208.3 kB/s eta 0:00:00\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-18.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyarrow     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "#from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "from pyparsing import col\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import StandardScalerModel\n",
    "#from  model.process import DataProcessor\n",
    "import os\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, scaler_model_path=\"scaler\", feature_columns=None):\n",
    "        \"\"\"\n",
    "        Initialize the DataProcessor class.\n",
    "        :param scaler_model_path: Path to the pre-trained scaler model.\n",
    "        :param feature_columns: List of feature column names to be used for modeling.\n",
    "        \"\"\"\n",
    "        self.feature_columns = feature_columns if feature_columns is not None else [\n",
    "            'INCOME_PER_FAM_MEMBER', 'AMT_INCOME_TOTAL', 'YEARS_EMPLOYED', 'AGE', 'CREDIT_HISTORY_LENGTH', 'RECENT_ACTIVITY'\n",
    "        ]\n",
    "        self.scaler_model = StandardScalerModel.load(scaler_model_path)\n",
    "\n",
    "    def process(self, input_df, output_dir):\n",
    "        \"\"\"\n",
    "        This method processes the input data, applies all transformations, \n",
    "        and saves the output to the specified directory.\n",
    "        :param input_df: Spark DataFrame to be processed.\n",
    "        :param output_dir: Directory to save the processed data.\n",
    "        :param file_format: File format to save the data (default is \"parquet\").\n",
    "        \"\"\"\n",
    "        # Ordinal Encoding for specific categorical columns\n",
    "        categorical_ordinal_columns = ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']\n",
    "        \n",
    "        # Apply StringIndexer for ordinal columns\n",
    "        indexers = [StringIndexer(inputCol=col, outputCol=col + \"_encoded\") for col in categorical_ordinal_columns]\n",
    "        pipeline = Pipeline(stages=indexers)\n",
    "        indexed_df = pipeline.fit(input_df).transform(input_df)\n",
    "\n",
    "        # OneHotEncoding for specific categorical columns\n",
    "        categorical_onehot_columns = ['NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE']\n",
    "        \n",
    "        # Apply StringIndexer for one-hot encoding\n",
    "        indexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\") for col in categorical_onehot_columns]\n",
    "        indexer_pipeline = Pipeline(stages=indexers)\n",
    "        indexed_onehot_df = indexer_pipeline.fit(input_df).transform(input_df)\n",
    "\n",
    "        # Check for distinct values before applying OneHotEncoder\n",
    "        valid_columns = []\n",
    "        for col in categorical_onehot_columns:\n",
    "            distinct_values = indexed_onehot_df.select(col + \"_index\").distinct().count()\n",
    "            if distinct_values >= 2:\n",
    "                valid_columns.append(col)\n",
    "\n",
    "        # Apply OneHotEncoder only to valid columns\n",
    "        encoder = OneHotEncoder(inputCols=[col + \"_index\" for col in valid_columns],\n",
    "                                outputCols=[col + \"_onehot\" for col in valid_columns])\n",
    "        encoded_df = encoder.fit(indexed_onehot_df).transform(indexed_onehot_df)\n",
    "        \n",
    "        # Custom feature engineering\n",
    "        input_df = input_df.withColumn(\"AGE\", F.abs(input_df['DAYS_BIRTH']) / 365)\n",
    "        input_df = input_df.withColumn(\"YEARS_EMPLOYED\", F.abs(input_df['DAYS_EMPLOYED']) / 365)\n",
    "\n",
    "        # Drop unnecessary columns\n",
    "        input_df = input_df.drop('DAYS_BIRTH', 'DAYS_EMPLOYED')\n",
    "\n",
    "        # Create new features (e.g., 'INCOME_PER_FAM_MEMBER')\n",
    "        input_df = input_df.withColumn(\"INCOME_PER_FAM_MEMBER\", input_df['AMT_INCOME_TOTAL'] / input_df['CNT_FAM_MEMBERS'])\n",
    "\n",
    "        # Feature engineering for 'CREDIT_HISTORY_LENGTH' and 'RECENT_ACTIVITY'\n",
    "        # Convert 'MONTHS_BALANCE' to numeric type (integer)\n",
    "        input_df = input_df.withColumn(\"MONTHS_BALANCE\", F.col(\"MONTHS_BALANCE\").cast(\"int\"))\n",
    "\n",
    "        # Now perform the subtraction operation\n",
    "        credit_history_length = input_df.groupBy('ID').agg(\n",
    "            (F.max('MONTHS_BALANCE') - F.min('MONTHS_BALANCE')).alias('CREDIT_HISTORY_LENGTH')\n",
    "        )\n",
    "        recent_activity_flag = input_df.groupBy('ID').agg(\n",
    "            (F.expr(\"max(CASE WHEN MONTHS_BALANCE >= -4 THEN 1 ELSE 0 END)\")).alias('RECENT_ACTIVITY')\n",
    "        )\n",
    "\n",
    "        # Join these features back into the main dataframe\n",
    "        input_df = input_df.join(credit_history_length, on=\"ID\", how=\"left\")\n",
    "        input_df = input_df.join(recent_activity_flag, on=\"ID\", how=\"left\")\n",
    "\n",
    "        # Select features for modeling\n",
    "        selected_features = ['INCOME_PER_FAM_MEMBER', 'AMT_INCOME_TOTAL', 'YEARS_EMPLOYED', 'AGE', 'CREDIT_HISTORY_LENGTH', 'RECENT_ACTIVITY']\n",
    "\n",
    "        # Create a VectorAssembler to combine features into a single vector column\n",
    "        assembler = VectorAssembler(inputCols=selected_features, outputCol=\"features\")\n",
    "\n",
    "        # Apply VectorAssembler to prepare features\n",
    "        feature_df = assembler.transform(input_df)\n",
    "\n",
    "        # Apply the pre-trained scaler model\n",
    "        train_data_scaled = self.scaler_model.transform(feature_df)\n",
    "\n",
    "        # Save the processed data\n",
    "        train_data_scaled.write.parquet(output_dir, mode='overwrite')\n",
    "        return train_data_scaled\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the trained model from Parquet\n",
    "def load_model(model_path=\"rf_model\"):\n",
    "    \"\"\"\n",
    "    Load the trained model stored in Parquet format.\n",
    "    \"\"\"\n",
    "    rf_model = RandomForestClassificationModel.load(model_path)\n",
    "    print(\"RandomForest model loaded successfully!\")\n",
    "    return rf_model\n",
    "\n",
    "sample_processed_directory = \"Model Development/sample/processed\"\n",
    "sample_raw_directory = \"Model Development/sample/raw\"\n",
    "\n",
    "\n",
    "# Process data using DataProcessor class\n",
    "def preprocess_data(raw_directory= sample_raw_directory,processed_directory= sample_processed_directory):\n",
    "    \n",
    "    sample_data = spark.read.parquet(raw_directory)\n",
    "    data_processor = DataProcessor()\n",
    "    sample_processed = data_processor.process(input_df=sample_data,output_dir=processed_directory)\n",
    "    #sample_processed.show()\n",
    "    return sample_processed\n",
    "\n",
    "# predict from a given directory, require a parquet format for both data and model\n",
    "def predict(processed_directory: str, model:pyspark.ml.classification.RandomForestClassificationModel) -> dict:\n",
    "    \"\"\"\n",
    "    Predict using the loaded model.\n",
    "    :param features: Input features for prediction.\n",
    "    :param model: The loaded model.\n",
    "    :return: Prediction result as a dictionary.\n",
    "    \"\"\"\n",
    "    test_data_loaded = spark.read.parquet(processed_directory)\n",
    "\n",
    "    # Convert input features to a Spark DataFrame\n",
    "    #feature_df = spark.createDataFrame([features])\n",
    "    \n",
    "    # Apply the model\n",
    "    prediction = model.transform(test_data_loaded)\n",
    "    prediction_result = prediction.select(\"prediction\").collect()[0][0]\n",
    "    \n",
    "    return {\"prediction\": prediction_result}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dictr={\n",
    "  \"ID\": 5111095,\n",
    "  \"CODE_GENDER\": \"F\",\n",
    "  \"FLAG_OWN_CAR\": \"N\",\n",
    "  \"FLAG_OWN_REALTY\": \"N\",\n",
    "  \"CNT_CHILDREN\": 0,\n",
    "  \"AMT_INCOME_TOTAL\": 112500.0,\n",
    "  \"NAME_INCOME_TYPE\": \"Working\",\n",
    "  \"NAME_EDUCATION_TYPE\": \"Secondary / secondary special\",\n",
    "  \"NAME_FAMILY_STATUS\": \"Married\",\n",
    "  \"NAME_HOUSING_TYPE\": \"House / apartment\",\n",
    "  \"DAYS_BIRTH\": -10653,\n",
    "  \"DAYS_EMPLOYED\": -3007,\n",
    "  \"FLAG_MOBIL\": 1,\n",
    "  \"FLAG_WORK_PHONE\": 0,\n",
    "  \"FLAG_PHONE\": 1,\n",
    "  \"FLAG_EMAIL\": 0,\n",
    "  \"OCCUPATION_TYPE\": \"Core staff\",\n",
    "  \"CNT_FAM_MEMBERS\": 2.0,\n",
    "  \"MONTHS_BALANCE\": -6,\n",
    "  \"STATUS\": \"C\",\n",
    "  \"mode_status\": \"C\"\n",
    "}\n",
    "input_data = pd.DataFrame([dictr])\n",
    "\n",
    "# Save the input data as a Parquet file\n",
    "#parquet_path = \"/temp_data/input_data.parquet\"\n",
    "input_data.to_parquet(\"testfolder/raw\", engine='pyarrow', index=False)\n",
    "        \n",
    "# preprocess data\n",
    "data_processed= preprocess_data(\"testfolder/raw\",\"testfolder/processed\")\n",
    "        \n",
    "# load the model\n",
    "model= load_model()\n",
    "\n",
    "        \n",
    "# # Load the test data\n",
    "# test_data_loaded = spark.read.parquet(sample_processed_directory)\n",
    "        \n",
    "# Make the prediction\n",
    "rf_predictions = predict(\"testfolder/processed\", model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
