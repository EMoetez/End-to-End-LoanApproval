{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/07 15:59:45 WARN Utils: Your hostname, omar-HP-Laptop-15-dw2xxx resolves to a loopback address: 127.0.1.1; using 192.168.1.21 instead (on interface wlo1)\n",
      "24/12/07 15:59:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/07 15:59:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/07 15:59:48 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CreditCardApprovalPrediction\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\n",
      "|     ID|CODE_GENDER|FLAG_OWN_CAR|FLAG_OWN_REALTY|CNT_CHILDREN|AMT_INCOME_TOTAL|    NAME_INCOME_TYPE| NAME_EDUCATION_TYPE|  NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|DAYS_BIRTH|DAYS_EMPLOYED|FLAG_MOBIL|FLAG_WORK_PHONE|FLAG_PHONE|FLAG_EMAIL|OCCUPATION_TYPE|CNT_FAM_MEMBERS|\n",
      "+-------+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\n",
      "|5008804|          M|           Y|              Y|           0|        427500.0|             Working|    Higher education|      Civil marriage| Rented apartment|    -12005|        -4542|         1|              1|         0|         0|           NULL|            2.0|\n",
      "|5008805|          M|           Y|              Y|           0|        427500.0|             Working|    Higher education|      Civil marriage| Rented apartment|    -12005|        -4542|         1|              1|         0|         0|           NULL|            2.0|\n",
      "|5008806|          M|           Y|              Y|           0|        112500.0|             Working|Secondary / secon...|             Married|House / apartment|    -21474|        -1134|         1|              0|         0|         0| Security staff|            2.0|\n",
      "|5008808|          F|           N|              Y|           0|        270000.0|Commercial associate|Secondary / secon...|Single / not married|House / apartment|    -19110|        -3051|         1|              0|         1|         1|    Sales staff|            1.0|\n",
      "|5008809|          F|           N|              Y|           0|        270000.0|Commercial associate|Secondary / secon...|Single / not married|House / apartment|    -19110|        -3051|         1|              0|         1|         1|    Sales staff|            1.0|\n",
      "+-------+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+--------------+------+\n",
      "|     ID|MONTHS_BALANCE|STATUS|\n",
      "+-------+--------------+------+\n",
      "|5001711|             0|     X|\n",
      "|5001711|            -1|     0|\n",
      "|5001711|            -2|     0|\n",
      "|5001711|            -3|     0|\n",
      "|5001712|             0|     C|\n",
      "+-------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define file paths\n",
    "path_to_application = 'data/raw/application_record.csv'\n",
    "path_to_credit = 'data/raw/credit_record.csv'\n",
    "\n",
    "# Ingest data into Spark DataFrames\n",
    "applicant_data_spark = spark.read.option(\"header\", \"true\").csv(path_to_application, inferSchema=True)\n",
    "credit_data_spark = spark.read.option(\"header\", \"true\").csv(path_to_credit, inferSchema=True)\n",
    "\n",
    "# Show first few rows for verification\n",
    "applicant_data_sparsample_datak.show(5)\n",
    "credit_data_spark.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/07 15:59:59 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/12/07 16:00:02 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+-----------+-----+--------------+------+\n",
      "|     ID|CODE_GENDER|FLAG_OWN_CAR|FLAG_OWN_REALTY|CNT_CHILDREN|AMT_INCOME_TOTAL|NAME_INCOME_TYPE| NAME_EDUCATION_TYPE|NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|DAYS_BIRTH|DAYS_EMPLOYED|FLAG_MOBIL|FLAG_WORK_PHONE|FLAG_PHONE|FLAG_EMAIL|OCCUPATION_TYPE|CNT_FAM_MEMBERS|mode_status|label|MONTHS_BALANCE|STATUS|\n",
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+-----------+-----+--------------+------+\n",
      "|5008806|          M|           Y|              Y|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|    -21474|        -1134|         1|              0|         0|         0| Security staff|            2.0|          C|    1|             0|     C|\n",
      "|5008806|          M|           Y|              Y|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|    -21474|        -1134|         1|              0|         0|         0| Security staff|            2.0|          C|    1|            -1|     C|\n",
      "|5008806|          M|           Y|              Y|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|    -21474|        -1134|         1|              0|         0|         0| Security staff|            2.0|          C|    1|            -2|     C|\n",
      "|5008806|          M|           Y|              Y|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|    -21474|        -1134|         1|              0|         0|         0| Security staff|            2.0|          C|    1|            -3|     C|\n",
      "|5008806|          M|           Y|              Y|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|    -21474|        -1134|         1|              0|         0|         0| Security staff|            2.0|          C|    1|            -4|     C|\n",
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+-----------+-----+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, count, lit, min, max, abs\n",
    "\n",
    "# Drop duplicates\n",
    "applicant_data_spark = applicant_data_spark.dropDuplicates(['ID'])\n",
    "\n",
    "# Mode calculation for 'STATUS'\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Group by 'ID' and calculate mode (most frequent value) for 'STATUS'\n",
    "status_mode_df = credit_data_spark.groupBy('ID').agg(\n",
    "    F.expr(\"first(STATUS)\").alias(\"mode_status\")  # Simplified mode calculation in Spark\n",
    ")\n",
    "\n",
    "# Filter out 'X' values\n",
    "status_mode_df = status_mode_df.filter(status_mode_df['mode_status'] != 'X')\n",
    "\n",
    "# Merge applicant data with status mode\n",
    "merged_spark_df = applicant_data_spark.join(status_mode_df, on=\"ID\", how=\"inner\")\n",
    "\n",
    "# Apply the label encoding logic to 'mode_status'\n",
    "merged_spark_df = merged_spark_df.withColumn(\n",
    "    \"label\", when(merged_spark_df[\"mode_status\"].isin(\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"), 0)\n",
    "    .when(merged_spark_df[\"mode_status\"] == \"C\", 1)\n",
    "    .otherwise(lit(None))\n",
    ")\n",
    "merged_spark_df = merged_spark_df.join(credit_data_spark, on=\"ID\", how=\"left\")\n",
    "\n",
    "\n",
    "# Show the DataFrame after transformations\n",
    "merged_spark_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding and Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+-----------+-----+--------------+------+-------------------+--------------------+-----------------------+\n",
      "|     ID|CODE_GENDER|FLAG_OWN_CAR|FLAG_OWN_REALTY|CNT_CHILDREN|AMT_INCOME_TOTAL|NAME_INCOME_TYPE| NAME_EDUCATION_TYPE|NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|DAYS_BIRTH|DAYS_EMPLOYED|FLAG_MOBIL|FLAG_WORK_PHONE|FLAG_PHONE|FLAG_EMAIL|OCCUPATION_TYPE|CNT_FAM_MEMBERS|mode_status|label|MONTHS_BALANCE|STATUS|CODE_GENDER_encoded|FLAG_OWN_CAR_encoded|FLAG_OWN_REALTY_encoded|\n",
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+-----------+-----+--------------+------+-------------------+--------------------+-----------------------+\n",
      "|5008806|          M|           Y|              Y|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|    -21474|        -1134|         1|              0|         0|         0| Security staff|            2.0|          C|    1|             0|     C|                1.0|                 1.0|                    0.0|\n",
      "|5008806|          M|           Y|              Y|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|    -21474|        -1134|         1|              0|         0|         0| Security staff|            2.0|          C|    1|            -1|     C|                1.0|                 1.0|                    0.0|\n",
      "|5008806|          M|           Y|              Y|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|    -21474|        -1134|         1|              0|         0|         0| Security staff|            2.0|          C|    1|            -2|     C|                1.0|                 1.0|                    0.0|\n",
      "|5008806|          M|           Y|              Y|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|    -21474|        -1134|         1|              0|         0|         0| Security staff|            2.0|          C|    1|            -3|     C|                1.0|                 1.0|                    0.0|\n",
      "|5008806|          M|           Y|              Y|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|    -21474|        -1134|         1|              0|         0|         0| Security staff|            2.0|          C|    1|            -4|     C|                1.0|                 1.0|                    0.0|\n",
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+-----------+-----+--------------+------+-------------------+--------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Ordinal Encoding for categorical columns\n",
    "categorical_ordinal_columns = ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']\n",
    "\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_encoded\") for col in categorical_ordinal_columns]\n",
    "\n",
    "# Apply StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "indexed_df = pipeline.fit(merged_spark_df).transform(merged_spark_df)\n",
    "\n",
    "indexed_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 94:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+-----------+-----+--------------+------+----------------------+-------------------------+------------------------+-----------------------+-----------------------+--------------------------+-------------------------+------------------------+\n",
      "|     ID|CODE_GENDER|FLAG_OWN_CAR|FLAG_OWN_REALTY|CNT_CHILDREN|AMT_INCOME_TOTAL|NAME_INCOME_TYPE| NAME_EDUCATION_TYPE|NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|DAYS_BIRTH|DAYS_EMPLOYED|FLAG_MOBIL|FLAG_WORK_PHONE|FLAG_PHONE|FLAG_EMAIL|OCCUPATION_TYPE|CNT_FAM_MEMBERS|mode_status|label|MONTHS_BALANCE|STATUS|NAME_INCOME_TYPE_index|NAME_EDUCATION_TYPE_index|NAME_FAMILY_STATUS_index|NAME_HOUSING_TYPE_index|NAME_INCOME_TYPE_onehot|NAME_EDUCATION_TYPE_onehot|NAME_FAMILY_STATUS_onehot|NAME_HOUSING_TYPE_onehot|\n",
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+-----------+-----+--------------+------+----------------------+-------------------------+------------------------+-----------------------+-----------------------+--------------------------+-------------------------+------------------------+\n",
      "|5008806|          M|           Y|              Y|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|    -21474|        -1134|         1|              0|         0|         0| Security staff|            2.0|          C|    1|             0|     C|                   0.0|                      0.0|                     0.0|                    0.0|          (4,[0],[1.0])|             (4,[0],[1.0])|            (4,[0],[1.0])|           (5,[0],[1.0])|\n",
      "|5008806|          M|           Y|              Y|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|    -21474|        -1134|         1|              0|         0|         0| Security staff|            2.0|          C|    1|            -1|     C|                   0.0|                      0.0|                     0.0|                    0.0|          (4,[0],[1.0])|             (4,[0],[1.0])|            (4,[0],[1.0])|           (5,[0],[1.0])|\n",
      "|5008806|          M|           Y|              Y|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|    -21474|        -1134|         1|              0|         0|         0| Security staff|            2.0|          C|    1|            -2|     C|                   0.0|                      0.0|                     0.0|                    0.0|          (4,[0],[1.0])|             (4,[0],[1.0])|            (4,[0],[1.0])|           (5,[0],[1.0])|\n",
      "|5008806|          M|           Y|              Y|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|    -21474|        -1134|         1|              0|         0|         0| Security staff|            2.0|          C|    1|            -3|     C|                   0.0|                      0.0|                     0.0|                    0.0|          (4,[0],[1.0])|             (4,[0],[1.0])|            (4,[0],[1.0])|           (5,[0],[1.0])|\n",
      "|5008806|          M|           Y|              Y|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|    -21474|        -1134|         1|              0|         0|         0| Security staff|            2.0|          C|    1|            -4|     C|                   0.0|                      0.0|                     0.0|                    0.0|          (4,[0],[1.0])|             (4,[0],[1.0])|            (4,[0],[1.0])|           (5,[0],[1.0])|\n",
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+-----------+-----+--------------+------+----------------------+-------------------------+------------------------+-----------------------+-----------------------+--------------------------+-------------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# One-hot encoding for categorical columns\n",
    "categorical_onehot_columns = ['NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE']\n",
    "\n",
    "# First, use StringIndexer to convert to numeric values\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\") for col in categorical_onehot_columns]\n",
    "indexer_pipeline = Pipeline(stages=indexers)\n",
    "indexed_onehot_df = indexer_pipeline.fit(merged_spark_df).transform(merged_spark_df)\n",
    "\n",
    "# Now apply OneHotEncoder\n",
    "encoder = OneHotEncoder(inputCols=[col + \"_index\" for col in categorical_onehot_columns], \n",
    "                        outputCols=[col + \"_onehot\" for col in categorical_onehot_columns])\n",
    "\n",
    "encoded_df = encoder.fit(indexed_onehot_df).transform(indexed_onehot_df)\n",
    "encoded_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 111:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+---------------+----------+----------+---------------+---------------+-----------+-----+--------------+------+-----------------+-----------------+---------------------+---------------------+---------------+\n",
      "|     ID|CODE_GENDER|FLAG_OWN_CAR|FLAG_OWN_REALTY|CNT_CHILDREN|AMT_INCOME_TOTAL|NAME_INCOME_TYPE| NAME_EDUCATION_TYPE|NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|FLAG_MOBIL|FLAG_WORK_PHONE|FLAG_PHONE|FLAG_EMAIL|OCCUPATION_TYPE|CNT_FAM_MEMBERS|mode_status|label|MONTHS_BALANCE|STATUS|              AGE|   YEARS_EMPLOYED|INCOME_PER_FAM_MEMBER|CREDIT_HISTORY_LENGTH|RECENT_ACTIVITY|\n",
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+---------------+----------+----------+---------------+---------------+-----------+-----+--------------+------+-----------------+-----------------+---------------------+---------------------+---------------+\n",
      "|5008806|          M|           Y|              Y|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|         1|              0|         0|         0| Security staff|            2.0|          C|    1|             0|     C|58.83287671232877|3.106849315068493|              56250.0|                   29|              1|\n",
      "|5008806|          M|           Y|              Y|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|         1|              0|         0|         0| Security staff|            2.0|          C|    1|            -1|     C|58.83287671232877|3.106849315068493|              56250.0|                   29|              1|\n",
      "|5008806|          M|           Y|              Y|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|         1|              0|         0|         0| Security staff|            2.0|          C|    1|            -2|     C|58.83287671232877|3.106849315068493|              56250.0|                   29|              1|\n",
      "|5008806|          M|           Y|              Y|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|         1|              0|         0|         0| Security staff|            2.0|          C|    1|            -3|     C|58.83287671232877|3.106849315068493|              56250.0|                   29|              1|\n",
      "|5008806|          M|           Y|              Y|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|         1|              0|         0|         0| Security staff|            2.0|          C|    1|            -4|     C|58.83287671232877|3.106849315068493|              56250.0|                   29|              1|\n",
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+---------------+----------+----------+---------------+---------------+-----------+-----+--------------+------+-----------------+-----------------+---------------------+---------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Custom feature engineering\n",
    "merged_spark_df = merged_spark_df.withColumn(\"AGE\", abs(merged_spark_df['DAYS_BIRTH']) / 365)\n",
    "merged_spark_df = merged_spark_df.withColumn(\"YEARS_EMPLOYED\", abs(merged_spark_df['DAYS_EMPLOYED']) / 365)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "merged_spark_df = merged_spark_df.drop('DAYS_BIRTH', 'DAYS_EMPLOYED')\n",
    "\n",
    "# Create new features (e.g., 'INCOME_PER_FAM_MEMBER')\n",
    "merged_spark_df = merged_spark_df.withColumn(\"INCOME_PER_FAM_MEMBER\", merged_spark_df['AMT_INCOME_TOTAL'] / merged_spark_df['CNT_FAM_MEMBERS'])\n",
    "\n",
    "\n",
    "# Feature engineering for 'CREDIT_HISTORY_LENGTH' and 'RECENT_ACTIVITY'\n",
    "credit_history_length = merged_spark_df.groupBy('ID').agg(\n",
    "    (max('MONTHS_BALANCE') - min('MONTHS_BALANCE')).alias('CREDIT_HISTORY_LENGTH')\n",
    ")\n",
    "recent_activity_flag = merged_spark_df.groupBy('ID').agg(\n",
    "    (F.expr(\"max(CASE WHEN MONTHS_BALANCE >= -4 THEN 1 ELSE 0 END)\")).alias('RECENT_ACTIVITY')\n",
    ")\n",
    "\n",
    "# Join these features back into the main dataframe\n",
    "merged_spark_df = merged_spark_df.join(credit_history_length, on=\"ID\", how=\"left\")\n",
    "merged_spark_df = merged_spark_df.join(recent_activity_flag, on=\"ID\", how=\"left\")\n",
    "\n",
    "merged_spark_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation (Train/Test Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 161:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+---------------+----------+----------+---------------+---------------+-----------+-----+--------------+------+-----------------+-----------------+---------------------+---------------------+---------------+--------------------+--------------------+\n",
      "|     ID|CODE_GENDER|FLAG_OWN_CAR|FLAG_OWN_REALTY|CNT_CHILDREN|AMT_INCOME_TOTAL|NAME_INCOME_TYPE| NAME_EDUCATION_TYPE|NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|FLAG_MOBIL|FLAG_WORK_PHONE|FLAG_PHONE|FLAG_EMAIL|OCCUPATION_TYPE|CNT_FAM_MEMBERS|mode_status|label|MONTHS_BALANCE|STATUS|              AGE|   YEARS_EMPLOYED|INCOME_PER_FAM_MEMBER|CREDIT_HISTORY_LENGTH|RECENT_ACTIVITY|            features|     scaled_features|\n",
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+---------------+----------+----------+---------------+---------------+-----------+-----+--------------+------+-----------------+-----------------+---------------------+---------------------+---------------+--------------------+--------------------+\n",
      "|5008806|          M|           Y|              Y|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|         1|              0|         0|         0| Security staff|            2.0|          C|    1|           -29|     X|58.83287671232877|3.106849315068493|              56250.0|                   29|              1|[56250.0,112500.0...|[0.78590430402864...|\n",
      "|5008806|          M|           Y|              Y|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|         1|              0|         0|         0| Security staff|            2.0|          C|    1|           -28|     0|58.83287671232877|3.106849315068493|              56250.0|                   29|              1|[56250.0,112500.0...|[0.78590430402864...|\n",
      "|5008806|          M|           Y|              Y|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|         1|              0|         0|         0| Security staff|            2.0|          C|    1|           -26|     X|58.83287671232877|3.106849315068493|              56250.0|                   29|              1|[56250.0,112500.0...|[0.78590430402864...|\n",
      "|5008806|          M|           Y|              Y|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|         1|              0|         0|         0| Security staff|            2.0|          C|    1|           -25|     X|58.83287671232877|3.106849315068493|              56250.0|                   29|              1|[56250.0,112500.0...|[0.78590430402864...|\n",
      "|5008806|          M|           Y|              Y|           0|        112500.0|         Working|Secondary / secon...|           Married|House / apartment|         1|              0|         0|         0| Security staff|            2.0|          C|    1|           -24|     0|58.83287671232877|3.106849315068493|              56250.0|                   29|              1|[56250.0,112500.0...|[0.78590430402864...|\n",
      "+-------+-----------+------------+---------------+------------+----------------+----------------+--------------------+------------------+-----------------+----------+---------------+----------+----------+---------------+---------------+-----------+-----+--------------+------+-----------------+-----------------+---------------------+---------------------+---------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "# Select features for modeling\n",
    "selected_features = ['INCOME_PER_FAM_MEMBER', 'AMT_INCOME_TOTAL', 'YEARS_EMPLOYED', 'AGE', 'CREDIT_HISTORY_LENGTH', 'RECENT_ACTIVITY']\n",
    "\n",
    "# Create a VectorAssembler to combine features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=selected_features, outputCol=\"features\")\n",
    "\n",
    "# Apply VectorAssembler to prepare features\n",
    "feature_df = assembler.transform(merged_spark_df)\n",
    "\n",
    "# Split data into training and test sets (70% train, 30% test)\n",
    "train_data, test_data = feature_df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "scaler_model = scaler.fit(train_data)\n",
    "train_data_scaled = scaler_model.transform(train_data)\n",
    "test_data_scaled = scaler_model.transform(test_data)\n",
    "scaler_model.save(\"scaler\")\n",
    "train_data_scaled.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_scaled saved to: data/processed/train_data_scaled.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 193:==============>                                          (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data_scaled saved to: data/processed/test_data_scaled.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define the paths where you want to save the processed data\n",
    "train_data_path = \"data/processed/train_data_scaled.parquet\"\n",
    "test_data_path = \"data/processed/test_data_scaled.parquet\"\n",
    "\n",
    "# Save train_data_scaled as a Parquet file\n",
    "train_data_scaled.write.parquet(train_data_path)\n",
    "print(f\"train_data_scaled saved to: {train_data_path}\")\n",
    "\n",
    "# Save test_data_scaled as a Parquet file\n",
    "test_data_scaled.write.parquet(test_data_path)\n",
    "print(f\"test_data_scaled saved to: {test_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/07 10:09:41 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.62      0.66      2551\n",
      "           1       0.83      0.87      0.85      5251\n",
      "\n",
      "    accuracy                           0.79      7802\n",
      "   macro avg       0.76      0.75      0.75      7802\n",
      "weighted avg       0.78      0.79      0.78      7802\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score: 0.745055934166765\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Train Logistic Regression Model\n",
    "lr = LogisticRegression(featuresCol='scaled_features', labelCol='label')\n",
    "lr_model = lr.fit(train_data_scaled)\n",
    "\n",
    "# Predictions\n",
    "predictions = lr_model.transform(test_data_scaled)\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "predictions_pd = predictions.select(\"label\", \"prediction\").toPandas()\n",
    "\n",
    "# Classification Report\n",
    "y_true = predictions_pd['label']\n",
    "y_pred = predictions_pd['prediction']\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# AUC Score\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\")\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "print(f\"ROC AUC Score: {roc_auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/07 16:05:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:05:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:05:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:05:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:05:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:05:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:05:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:05:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:05:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:05:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:05:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:05:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:06:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:06:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:06:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:06:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:06:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:06:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:06:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:06:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:06:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:06:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:06:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:06:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:06:12 WARN MemoryStore: Not enough space to cache rdd_814_2 in memory! (computed 30.6 MiB so far)\n",
      "24/12/07 16:06:12 WARN BlockManager: Persisting block rdd_814_2 to disk instead.\n",
      "24/12/07 16:06:12 WARN MemoryStore: Not enough space to cache rdd_814_3 in memory! (computed 30.6 MiB so far)\n",
      "24/12/07 16:06:12 WARN BlockManager: Persisting block rdd_814_3 to disk instead.\n",
      "24/12/07 16:06:12 WARN MemoryStore: Not enough space to cache rdd_814_1 in memory! (computed 30.6 MiB so far)\n",
      "24/12/07 16:06:12 WARN BlockManager: Persisting block rdd_814_1 to disk instead.\n",
      "24/12/07 16:06:44 WARN DAGScheduler: Broadcasting large task binary with size 1143.9 KiB\n",
      "24/12/07 16:06:56 WARN DAGScheduler: Broadcasting large task binary with size 1957.2 KiB\n",
      "24/12/07 16:07:15 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "24/12/07 16:07:30 WARN DAGScheduler: Broadcasting large task binary with size 5.9 MiB\n",
      "24/12/07 16:07:54 WARN DAGScheduler: Broadcasting large task binary with size 1431.0 KiB\n",
      "24/12/07 16:07:55 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/12/07 16:08:17 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/12/07 16:08:40 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/12/07 16:08:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:08:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:08:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:08:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:08:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:08:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest AUC Score: 0.646850914468174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/07 16:08:59 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/12/07 16:09:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:09:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:09:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:09:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:09:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/12/07 16:09:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.32      0.45     34098\n",
      "           1       0.86      0.98      0.92    150697\n",
      "\n",
      "    accuracy                           0.85    184795\n",
      "   macro avg       0.81      0.65      0.68    184795\n",
      "weighted avg       0.84      0.85      0.83    184795\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Define the Random Forest Model\n",
    "rf = RandomForestClassifier(featuresCol='scaled_features', labelCol='label', numTrees=100, maxDepth=10)\n",
    "\n",
    "# Fit the model to the training data\n",
    "rf_model = rf.fit(train_data_scaled)\n",
    "\n",
    "# Generate predictions on the test data\n",
    "rf_predictions = rf_model.transform(test_data_scaled)\n",
    "\n",
    "# Evaluate using BinaryClassificationEvaluator (AUC)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\")\n",
    "rf_auc = evaluator.evaluate(rf_predictions)\n",
    "print(f\"Random Forest AUC Score: {rf_auc}\")\n",
    "\n",
    "# Convert predictions to Pandas DataFrame for classification report\n",
    "rf_predictions_pd = rf_predictions.select(\"label\", \"prediction\").toPandas()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_true_rf = rf_predictions_pd['label']\n",
    "y_pred_rf = rf_predictions_pd['prediction']\n",
    "\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_true_rf, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/07 16:10:40 WARN TaskSetManager: Stage 353 contains a task of very large size (1459 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 353:>                                                        (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest model saved to: rf_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model_path = \"rf_model\"  \n",
    "rf_model.save(model_path)\n",
    "print(f\"RandomForest model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 363:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest model loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "\n",
    "rf_model = RandomForestClassificationModel.load(model_path)\n",
    "print(\"RandomForest model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+---------------+----------+----------+---------------+---------------+-----------+-----+--------------+------+-----------------+----------------+---------------------+---------------------+---------------+--------------------+--------------------+\n",
      "|     ID|CODE_GENDER|FLAG_OWN_CAR|FLAG_OWN_REALTY|CNT_CHILDREN|AMT_INCOME_TOTAL|    NAME_INCOME_TYPE| NAME_EDUCATION_TYPE|  NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|FLAG_MOBIL|FLAG_WORK_PHONE|FLAG_PHONE|FLAG_EMAIL|OCCUPATION_TYPE|CNT_FAM_MEMBERS|mode_status|label|MONTHS_BALANCE|STATUS|              AGE|  YEARS_EMPLOYED|INCOME_PER_FAM_MEMBER|CREDIT_HISTORY_LENGTH|RECENT_ACTIVITY|            features|     scaled_features|\n",
      "+-------+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+---------------+----------+----------+---------------+---------------+-----------+-----+--------------+------+-----------------+----------------+---------------------+---------------------+---------------+--------------------+--------------------+\n",
      "|5008811|          F|           N|              Y|           0|        270000.0|Commercial associate|Secondary / secon...|Single / not married|House / apartment|         1|              0|         1|         1|    Sales staff|            1.0|          C|    1|           -38|     X|52.35616438356164|8.35890410958904|             270000.0|                   38|              1|[270000.0,270000....|[3.77234065933750...|\n",
      "|5008811|          F|           N|              Y|           0|        270000.0|Commercial associate|Secondary / secon...|Single / not married|House / apartment|         1|              0|         1|         1|    Sales staff|            1.0|          C|    1|           -36|     0|52.35616438356164|8.35890410958904|             270000.0|                   38|              1|[270000.0,270000....|[3.77234065933750...|\n",
      "|5008811|          F|           N|              Y|           0|        270000.0|Commercial associate|Secondary / secon...|Single / not married|House / apartment|         1|              0|         1|         1|    Sales staff|            1.0|          C|    1|           -32|     0|52.35616438356164|8.35890410958904|             270000.0|                   38|              1|[270000.0,270000....|[3.77234065933750...|\n",
      "|5008811|          F|           N|              Y|           0|        270000.0|Commercial associate|Secondary / secon...|Single / not married|House / apartment|         1|              0|         1|         1|    Sales staff|            1.0|          C|    1|           -30|     0|52.35616438356164|8.35890410958904|             270000.0|                   38|              1|[270000.0,270000....|[3.77234065933750...|\n",
      "|5008811|          F|           N|              Y|           0|        270000.0|Commercial associate|Secondary / secon...|Single / not married|House / apartment|         1|              0|         1|         1|    Sales staff|            1.0|          C|    1|           -16|     C|52.35616438356164|8.35890410958904|             270000.0|                   38|              1|[270000.0,270000....|[3.77234065933750...|\n",
      "+-------+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+---------------+----------+----------+---------------+---------------+-----------+-----+--------------+------+-----------------+----------------+---------------------+---------------------+---------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data_path = \"data/processed/test_data_scaled.parquet\"\n",
    "test_data = spark.read.parquet(test_data_path)\n",
    "test_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scaler import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaler loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScalerModel\n",
    "scaler = StandardScalerModel.load(\"scaler\")\n",
    "print(\"scaler loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/07 16:14:22 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest AUC Score: 0.646850914468174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/07 16:14:27 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.32      0.45     34098\n",
      "           1       0.86      0.98      0.92    150697\n",
      "\n",
      "    accuracy                           0.85    184795\n",
      "   macro avg       0.81      0.65      0.68    184795\n",
      "weighted avg       0.84      0.85      0.83    184795\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\")\n",
    "rf_auc = evaluator.evaluate(rf_predictions)\n",
    "print(f\"Random Forest AUC Score: {rf_auc}\")\n",
    "rf_predictions_pd = rf_predictions.select(\"label\", \"prediction\").toPandas()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_true_rf = rf_predictions_pd['label']\n",
    "y_pred_rf = rf_predictions_pd['prediction']\n",
    "\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_true_rf, y_pred_rf))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyarrow-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
